---
title: "Pseudo-Non-Linear Data Augmentation via Energy Minimization"
tags:
	- "Information Geometry"
	- "Data Augmentation"
date: 2024-09-07 00:00:00 -0500
priority: -20240907
path: "research/Pseudo-Non-Linear-Data-Augmentation-via-Energy-Minimization"
excerpt: "We propose a new non-linear data augmentation framework powered by information geometry."
selected: true
cover: "./preview.png"
links:
#   - name: "GitHub"
#     url: "https://github.com/sleepymalc/Travel-the-Same-Path"
  # - name: "arXiv"
  #   url: "https://arxiv.org/abs/2404.11577"
authors:
	- name: "**Pingbang Hu**"
	  url: "https://pbb.wtf/"
	- name: Mahito Sugiyama
	  url: "https://mahito.info/index_e.html"
---

## Brief Summary
<!--
How can we attribute the behaviors of machine learning models to their training data? While the classic *influence function*[^1] sheds light on the impact of individual samples, it often fails to capture the more complex and pronounced collective influence of a set of samples. To tackle this challenge, we study the Most Influential Subset Selection (MISS) problem, which aims to identify a subset of training samples with the greatest collective influence. We conduct a comprehensive analysis of the prevailing approaches in MISS, elucidating their strengths and weaknesses. Our findings reveal that influence-based greedy heuristics, a dominant class of algorithms in MISS, can provably fail even in linear regression. We delineate the failure modes, including the errors of influence function and the non-additive structure of the collective influence. Conversely, we demonstrate that an adaptive version of these heuristics which applies them iteratively, can effectively capture the interactions among samples and thus partially address the issues. Experiments on real-world datasets corroborate these theoretical findings, and further demonstrate that the merit of adaptivity can extend to more complex scenarios such as classification tasks and non-linear neural networks. We conclude our analysis by highlighting the inherent trade-off between performance and computational efficiency, and providing a range of discussions.

[^1]: <https://arxiv.org/abs/1703.04730> -->
